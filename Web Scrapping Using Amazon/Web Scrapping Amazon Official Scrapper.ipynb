{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ba07da",
   "metadata": {
    "papermill": {
     "duration": 0.00688,
     "end_time": "2021-12-10T14:00:58.883700",
     "exception": false,
     "start_time": "2021-12-10T14:00:58.876820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Web Scarpping Amazon for any product using user's input !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a2d33",
   "metadata": {
    "papermill": {
     "duration": 0.005764,
     "end_time": "2021-12-10T14:00:58.908484",
     "exception": false,
     "start_time": "2021-12-10T14:00:58.902720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is Web Scrapping?\n",
    "-  Web scraping is the process of using bots to extract content and data from a website. \n",
    "-  Unlike screen scraping, which only copies pixels displayed onscreen, web scraping extracts underlying HTML code and, with it, data stored in a database.\n",
    "-  Web scraping is used to collect large information from websites. \n",
    "\n",
    "# What Packages are Top packages used in Web Scrapping?\n",
    "-  Requests\n",
    "-  Beautiful Soup \n",
    "-  lxml\n",
    "-  Selenium\n",
    "-  Scrapy\n",
    "\n",
    "# Steps for Web Scrapping:\n",
    "-  Step 1: Find the URL that you want to scrape\n",
    "-  Step 2: Inspecting the Page\n",
    "-  Step 3: Find the data you want to extract\n",
    "-  Step 4: Write the code\n",
    "-  Step 5: Run the code and extract the data\n",
    "-  Step 6: Store the data in the required format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f17371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T14:00:58.924391Z",
     "iopub.status.busy": "2021-12-10T14:00:58.923210Z",
     "iopub.status.idle": "2021-12-10T14:00:58.927049Z",
     "shell.execute_reply": "2021-12-10T14:00:58.928133Z",
     "shell.execute_reply.started": "2021-12-10T14:00:12.991796Z"
    },
    "papermill": {
     "duration": 0.014262,
     "end_time": "2021-12-10T14:00:58.928448",
     "exception": false,
     "start_time": "2021-12-10T14:00:58.914186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comment: Install Following Packages for Web Scrapping:\n",
    "\n",
    "#Code:\n",
    "# !pip install selenium\n",
    "# !pip install webdriver_manager\n",
    "# !pip install webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5da12ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T14:00:58.944663Z",
     "iopub.status.busy": "2021-12-10T14:00:58.943487Z",
     "iopub.status.idle": "2021-12-10T14:00:58.950022Z",
     "shell.execute_reply": "2021-12-10T14:00:58.950603Z",
     "shell.execute_reply.started": "2021-12-10T14:00:12.996699Z"
    },
    "papermill": {
     "duration": 0.016104,
     "end_time": "2021-12-10T14:00:58.950771",
     "exception": false,
     "start_time": "2021-12-10T14:00:58.934667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#loading all the required library for webscrapping and saving the data into csv file.\n",
    "\n",
    "#Code:\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "def get_url(search_term):\n",
    "\n",
    "#     Comment:  Genrate URL for search term\n",
    "    template='https://www.amazon.com/s?k={}&ref=nb_sb_noss_2'\n",
    "    search_term=search_term.replace(\" \",\"+\")\n",
    "    \n",
    "    \n",
    "#     Comment: add term query to url\n",
    "    url=template.format(search_term)\n",
    "    \n",
    "#     Comment: Add page query placeholder\n",
    "    url+='&page{}'  \n",
    "    \n",
    "    return template.format(search_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6b7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_record(item):\n",
    "#     Comment: First we will take the decrpition\n",
    "\n",
    "    atag=item.h2.a\n",
    "    descrpition=atag.text.strip()\n",
    "    new_url='https://www.amazon.com'+atag.get('href')\n",
    "    \n",
    "#     Comment:  Price\n",
    "\n",
    "\n",
    "    try:\n",
    "        price_parent=item.find('span','a-price')\n",
    "        price=price_parent.find('span','a-offscreen').text\n",
    "    except AttributeError:\n",
    "        return\n",
    "\n",
    "#     Comment: Ranking and rating of the user \n",
    "\n",
    "    try:\n",
    "        rating=item.i.text\n",
    "        review_count=item.find('span',{'class':'a-size-base'}).text\n",
    "    except AttributeError:\n",
    "        rating=\"\"\n",
    "        review_count=\"\"\n",
    "        \n",
    "    result=(descrpition,price,rating,review_count,new_url)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7e9d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "    \n",
    "# #     Comment: this is a main function where we first load web driver\n",
    "\n",
    "#     from webdriver_manager.chrome import ChromeDriverManager\n",
    "# #     chrome_options = Options()\n",
    "# #     chrome_options.add_argument('--headless')\n",
    "# #     chrome_options.add_argument('--no-sandbox')\n",
    "# #     chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "#     driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    \n",
    "# #     Comment: Take Input parameter\n",
    "\n",
    "#     search_term=input('Enter the Keyword:')\n",
    "#     record=[]\n",
    "    \n",
    "# #     Comment: create url to search\n",
    "\n",
    "#     url=get_url(search_term)\n",
    "    \n",
    "#     for page in range(1,21):\n",
    "#         driver.get(url.format(page))\n",
    "#         soup=BeautifulSoup(driver.page_source,'html.parser')\n",
    "#         results=soup.find_all('div',{'data-component-type':'s-search-result'})\n",
    "        \n",
    "#         for item in results:\n",
    "#             u=extract_record(item)\n",
    "#             if u:\n",
    "#                 record.append(u)\n",
    "    \n",
    "#     driver.close()\n",
    "    \n",
    "#     df = pd.DataFrame(record, columns=['Descrpition','Price','Rating','ReviewCount','URL'])\n",
    "#     df.to_csv('FinalProductList.csv')\n",
    "     \n",
    "# #     Commnet: Or we can do it in another way\n",
    "\n",
    "#     with open(\"Productlists.csv\",'w',newline=\"\",encoding='utf-8') as f:\n",
    "#         writer=csv.writer(f)\n",
    "#         writer.writerow(['Descrpition','Price','Rating','ReviewCount','URL'])\n",
    "#         writer.writerow(record)\n",
    "#         print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b1a06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main2():\n",
    "    \n",
    "#     Comment: this is a main function where we first load web driver\n",
    "\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=chrome_options)\n",
    "#     option = webdriver.ChromeOptions()\n",
    "#     option.add_argument('headless')\n",
    "#     driver = webdriver.Chrome('chromedriver.exe')\n",
    "    \n",
    "#     Comment: Take Input parameter\n",
    "\n",
    "    search_term=input('Enter the Keyword:')\n",
    "    record=[]\n",
    "    \n",
    "#     Comment: create url to search\n",
    "\n",
    "    url=get_url(search_term)\n",
    "    \n",
    "    for page in range(1,21):\n",
    "        driver.get(url.format(page))\n",
    "        soup=BeautifulSoup(driver.page_source,'html.parser')\n",
    "        results=soup.find_all('div',{'data-component-type':'s-search-result'})\n",
    "        \n",
    "        for item in results:\n",
    "            u=extract_record(item)\n",
    "            if u:\n",
    "                record.append(u)\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    df = pd.DataFrame(record, columns=['Descrpition','Price','Rating','ReviewCount','URL'])\n",
    "    df.to_csv('FinalProductList.csv')\n",
    "     \n",
    "#     Commnet: Or we can do it in another way\n",
    "\n",
    "    with open(\"Productlists.csv\",'w',newline=\"\",encoding='utf-8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow(['Descrpition','Price','Rating','ReviewCount','URL'])\n",
    "        writer.writerow(record)\n",
    "        print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7456e2cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T14:00:58.967510Z",
     "iopub.status.busy": "2021-12-10T14:00:58.966411Z",
     "iopub.status.idle": "2021-12-10T14:00:58.968837Z",
     "shell.execute_reply": "2021-12-10T14:00:58.969316Z",
     "shell.execute_reply.started": "2021-12-10T14:00:13.011818Z"
    },
    "papermill": {
     "duration": 0.012846,
     "end_time": "2021-12-10T14:00:58.969527",
     "exception": false,
     "start_time": "2021-12-10T14:00:58.956681",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-098b1d4847dd>:10: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=chrome_options)\n",
      "<ipython-input-10-098b1d4847dd>:10: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=chrome_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Keyword:Iphone 12 mini\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Comment:Run this function to run whole program\n",
    "\n",
    "main2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db601d5b",
   "metadata": {
    "papermill": {
     "duration": 0.005638,
     "end_time": "2021-12-10T14:00:58.981278",
     "exception": false,
     "start_time": "2021-12-10T14:00:58.975640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 📌  **Note**: \n",
    "## This is a Prototype i have Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0bb08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.187576,
   "end_time": "2021-12-10T14:00:59.596844",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-10T14:00:48.409268",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
