{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ba07da",
   "metadata": {
    "papermill": {
     "duration": 0.00688,
     "end_time": "2021-12-10T14:00:58.883700",
     "exception": false,
     "start_time": "2021-12-10T14:00:58.876820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Web Scarpping Amazon for any product using user's input !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b17508f",
   "metadata": {
    "papermill": {
     "duration": 0.005552,
     "end_time": "2021-12-10T14:00:58.896944",
     "exception": false,
     "start_time": "2021-12-10T14:00:58.891392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/featured_image-6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a2d33",
   "metadata": {
    "papermill": {
     "duration": 0.005764,
     "end_time": "2021-12-10T14:00:58.908484",
     "exception": false,
     "start_time": "2021-12-10T14:00:58.902720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is Web Scrapping?\n",
    "-  Web scraping is the process of using bots to extract content and data from a website. \n",
    "-  Unlike screen scraping, which only copies pixels displayed onscreen, web scraping extracts underlying HTML code and, with it, data stored in a database.\n",
    "-  Web scraping is used to collect large information from websites. \n",
    "\n",
    "# What Packages are Top packages used in Web Scrapping?\n",
    "-  Requests\n",
    "-  Beautiful Soup \n",
    "-  lxml\n",
    "-  Selenium\n",
    "-  Scrapy\n",
    "\n",
    "# Steps for Web Scrapping:\n",
    "-  Step 1: Find the URL that you want to scrape\n",
    "-  Step 2: Inspecting the Page\n",
    "-  Step 3: Find the data you want to extract\n",
    "-  Step 4: Write the code\n",
    "-  Step 5: Run the code and extract the data\n",
    "-  Step 6: Store the data in the required format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f17371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T14:00:58.924391Z",
     "iopub.status.busy": "2021-12-10T14:00:58.923210Z",
     "iopub.status.idle": "2021-12-10T14:00:58.927049Z",
     "shell.execute_reply": "2021-12-10T14:00:58.928133Z",
     "shell.execute_reply.started": "2021-12-10T14:00:12.991796Z"
    },
    "papermill": {
     "duration": 0.014262,
     "end_time": "2021-12-10T14:00:58.928448",
     "exception": false,
     "start_time": "2021-12-10T14:00:58.914186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\programdata\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: urllib3[secure]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.4)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (0.19.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.3.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (20.3.0)\n",
      "Requirement already satisfied: outcome in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.5)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.0.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (2020.12.5)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (3.4.7)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (20.0.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n",
      "Collecting webdriver_manager\n",
      "  Downloading webdriver_manager-3.8.2-py2.py3-none-any.whl (26 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from webdriver_manager) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (1.26.4)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-0.20.0 webdriver-manager-3.8.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement webdriver\n",
      "ERROR: No matching distribution found for webdriver\n"
     ]
    }
   ],
   "source": [
    "# Comment: Install Following Packages for Web Scrapping:\n",
    "\n",
    "#Code:\n",
    "!pip install selenium\n",
    "!pip install webdriver_manager\n",
    "!pip install webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5da12ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T14:00:58.944663Z",
     "iopub.status.busy": "2021-12-10T14:00:58.943487Z",
     "iopub.status.idle": "2021-12-10T14:00:58.950022Z",
     "shell.execute_reply": "2021-12-10T14:00:58.950603Z",
     "shell.execute_reply.started": "2021-12-10T14:00:12.996699Z"
    },
    "papermill": {
     "duration": 0.016104,
     "end_time": "2021-12-10T14:00:58.950771",
     "exception": false,
     "start_time": "2021-12-10T14:00:58.934667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#loading all the required library for webscrapping and saving the data into csv file.\n",
    "\n",
    "#Code:\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_url(search_term):\n",
    "\n",
    "#     Comment:  Genrate URL for search term\n",
    "    template='https://www.amazon.com/s?k={}&ref=nb_sb_noss_2'\n",
    "    search_term=search_term.replace(\" \",\"+\")\n",
    "    \n",
    "    \n",
    "#     Comment: add term query to url\n",
    "    url=template.format(search_term)\n",
    "    \n",
    "#     Comment: Add page query placeholder\n",
    "    url+='&page{}'  \n",
    "    \n",
    "    return template.format(search_term)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96519c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_record(item):\n",
    "#     Comment: First we will take the decrpition\n",
    "\n",
    "    atag=item.h2.a\n",
    "    descrpition=atag.text.strip()\n",
    "    new_url='https://www.amazon.com'+atag.get('href')\n",
    "    \n",
    "#     Comment:  Price\n",
    "\n",
    "    try:\n",
    "        price_parent=item.find('span','a-price')\n",
    "        price=price_parent.find('span','a-offscreen').text\n",
    "    except AttributeError:\n",
    "        return\n",
    "\n",
    "#     Comment: Ranking and rating of the user \n",
    "\n",
    "    try:\n",
    "        rating=item.i.text\n",
    "        review_count=item.find('span',{'class':'a-size-base'}).text\n",
    "    except AttributeError:\n",
    "        rating=\"\"\n",
    "        review_count=\"\"\n",
    "        \n",
    "    result=(descrpition,price,rating,review_count,new_url)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb4b7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "#     Comment: this is a main function where we first load web driver\n",
    "\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    \n",
    "#     Comment: Take Input parameter\n",
    "\n",
    "    search_term=input('Enter the Keyword:')\n",
    "    record=[]\n",
    "    \n",
    "#     Comment: create url to search\n",
    "\n",
    "    url=get_url(search_term)\n",
    "    \n",
    "    for page in range(1,21):\n",
    "        driver.get(url.format(page))\n",
    "        soup=BeautifulSoup(driver.page_source,'html.parser')\n",
    "        results=soup.find_all('div',{'data-component-type':'s-search-result'})\n",
    "        \n",
    "        for item in results:\n",
    "            u=extract_record(item)\n",
    "            if u:\n",
    "                record.append(u)\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    df = pd.DataFrame(record, columns=['Descrpition','Price','Rating','ReviewCount','URL'])\n",
    "    df.to_csv('FinalProductList.csv')\n",
    "     \n",
    "#     Commnet: Or we can do it in another way\n",
    "\n",
    "    with open(\"Productlists.csv\",'w',newline=\"\",encoding='utf-8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow(['Descrpition','Price','Rating','ReviewCount','URL'])\n",
    "        writer.writerow(record)\n",
    "        print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7456e2cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T14:00:58.967510Z",
     "iopub.status.busy": "2021-12-10T14:00:58.966411Z",
     "iopub.status.idle": "2021-12-10T14:00:58.968837Z",
     "shell.execute_reply": "2021-12-10T14:00:58.969316Z",
     "shell.execute_reply.started": "2021-12-10T14:00:13.011818Z"
    },
    "papermill": {
     "duration": 0.012846,
     "end_time": "2021-12-10T14:00:58.969527",
     "exception": false,
     "start_time": "2021-12-10T14:00:58.956681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-e5223bb84863>:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Keyword:Xbox 360\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Comment:Run this function to run whole program\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db601d5b",
   "metadata": {
    "papermill": {
     "duration": 0.005638,
     "end_time": "2021-12-10T14:00:58.981278",
     "exception": false,
     "start_time": "2021-12-10T14:00:58.975640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 📌  **Note**: \n",
    "## This is a Prototype i have Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03a3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.187576,
   "end_time": "2021-12-10T14:00:59.596844",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-10T14:00:48.409268",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
